# 机器学习周志华 学习笔记

## 第一章 绪论

* 泛化能力
* 归纳偏好
* NFT(No Free Lunch Theorem)，所有算法对所有问题的期望是相同的，寓意需要结合具体案例选择适宜的算法求解。
* 发展历程

    1950，基于神经网络的“连接主义”(connectionism)，代表作为感知机(conceptron)。
    
    1960，基于逻辑表示的“符号主义”(symbolism)

    1980，机器学习成为一门独立的学科领域。

    1990，神经网络研究取得重大进展，BPNN出现，但参数设置缺乏理论指导。

    1995，统计学习(statistical learning)出场，代表作支持向量机(SVM)，一般化的核方法(kernel methods)。

    2000，深度学习浪潮涌来，设计语音、图像等应用广泛。火热原因：大数据，计算能力强，深度学习模型的参数众多，缺乏训练则容易过拟合；大数据时代，设备计算能力增强，数据储备和计算能力发展大。

    > 美国最尖端的技术由两大机构推进，NASA And DARPA。

## 第二章 模型评估与选择

### 2.1 经验误差与过拟合
    
    精度(accuracy)、训练误差(training error)、过拟合(overfitting)、欠拟合(underfitting)

### 2.2 评估方法

    * 留出法（hold-out），采用“分层采样”（stratified sampling）。
        > 一般可选择随机100次结果，返回结果的平均值。
        >
        > 常见方法：大约2/3-4/5用作训练。
    
    * 交叉验证法（cross validation）
        > 将数据集D划分为k个子集，子集之间不存在交集。然后，每次用k-1个子集并集作为训练集，余下的子集作为测试集。这样可获得k组测试结果，最终返回k个测试结果的均值。
        
        > 特例：留一法（leave-one-out）。即D中每个样本作为一个子集。
        
    * 自助法（bootstrapping）
    
        > 从m个样本中每次随机挑选一个后，加入D'中。该采样过程进行m次，得到和D相同大小的D'。由公式可知

$$\lim_{m\to\infty}(1-\frac{1}{m})\to\frac{1}{e}\approx0.368$$
        
        > D'中存在0.368概率的样本未被选中，因此可将D'作为训练集，D作为测试集。
        
        > 该方法适用于数据集较少的情况。
        
    * 调参与最终模型
        > 调参（parameter tuning），对参数可在一定范围内固定步长调整。
    
### 2.3 性能度量
    
    * 错误率与精度
    * 准确率（precision）与召回率（recall）
        > P-R 曲线
    
    * ROC（receiver operation characteristic）与AUC（area under-ROC curve） 

### 2.4 比较检验
    
    > 测试误差是否一定等于泛化误差？

### 2.5 偏差与方差（bias-variance decomposition）
    
    > 泛化误差可分解为偏差、方差与噪声之和。
    
$$E(f;D)=bias^2(x)+var(x)+\epsilon^2$$
    
    > 训练程度小时，偏差大，方差小，泛化误差大；
    >
    > 反之相反。
    
## 第三章 线性模型

### 3.1 基本形式

基本的数学表达形式如下所示：

$$f(x)=w^Tx+b$$

### 3.2 线性回归（linear regression）

令均方误差（mean square loss，MSE）最小，是回归任务的常用性能度量。

几何意义对应“欧式距离最小”（Euclidean distance），基于MSE进行模型求解的方法为“最小二乘法”（least square method）。

闭式解（closed-form）如下：

$$w = \frac{\sum_{i=1}^{m}y_{i}(x_{i}-\overline{x})}{\sum_{i=1}^{m}x_{i}^2-\frac{1}{m}(\sum_{i=1}^{m}x_{i})^2}$$

$$b=\frac{1}{m}\sum_{i=1}^{m}(y_{i}-wx_{i})$$

线性模型可变形，如“对数线性回归”（log-linear regression）。

$$lny=w^Tx+b$$

### 3.3 对数几率回归（logistic regression）

Sigmoid函数

$$y=\frac{1}{1+e^{-z}}$$

该函数对应为“对数几率回归”，是一种分类方法。

### 3.4 线性判别分析(Linear Discriminant analysis, LDA)

给定训练集,设法将样例投射在直线上,使同类样例的投影点尽可能靠近,异类样例的投影点尽可能远离.对新样本进行分类时,将其投影在相同的直线上,再根据投影点的位置确定新样本的类别.

### 3.5 多分类学习

定义
> 将二分类分类器推广到多分类器.

方法

* 一对一(OvO).把所有类别两两相互训练,共有N/(N-1)/2个情况.测试阶段,新样本提交给所有分类器,最终按照出现次数最多的类别为预测结果.

* 一对其余(OvR).把一个类别当作正例,剩余所有类别当作反例,共有N种情况.测试阶段,若仅为一种分类器是正例,其余是反例,则预测结果为该类别;若有多种正例,则根据置信度大的类别为分类结果.

* 多对多(MvM). 每次将多个类别做正例,多个类别做反例.编码阶段,产生M个训练集,可以训练出M种分类器.解码阶段,由测试结果进行比较,将最小距离作为最终分类结果.

示意图如下所示:

![image](http://github.com/tuchao1996/NoteMLZhihuaZhou/raw/master/img/fig1.png)

### 3.6 类别不平衡问题(class-imbalance)

问题定义如下:

> 数据集中正例或反例的比例,大大超出另外类别,该特点会对学习过程造成较大干扰.

当前解决方法主要有**三种方法**:

1. 分类器的决策规则如下:

$frac{y}{1-y}>1$

则为正例.

但是,当训练集的正例和反例的比例相差较大时,令$m^+$表示正例数量,$m^-$表示负例数目,则观测几率为$frac{m^+}{m^-}$.因此,只要满足以下条件,则为正例:

$frac{y}{1-y}>frac{m^+}{m^-}$

但是,决策通常是根据小节开头的公式,因此,需要令:

$frac{y^'}{1-y^'}=frac{y}{1-y}*frac{m^-}{m^+}$

这是类别不平衡学习的基本策略,**再缩放(rescaling)**

2. 欠采样(undersampling)

去除一些类别的样例,使得正反例的数量基本相同.

3. 过采样(oversampling)

增加一些样例,使得数据集的比例基本相同.